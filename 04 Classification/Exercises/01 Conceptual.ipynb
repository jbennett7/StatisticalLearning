{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55de980d-6f6c-4c7c-8825-b5c8750b292d",
   "metadata": {},
   "source": [
    "# Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925f6df-e898-4add-8ec6-165ed6bc33a4",
   "metadata": {},
   "source": [
    "__1.__ Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.  \n",
    "\n",
    "The logistic function\n",
    "\\begin{align}\\tag{4.2}\n",
    "p(X) = \\frac{ e^{\\beta_0 + \\beta_1 X } } { 1 + e^{ \\beta_0 + \\beta_1 X } }\n",
    "\\end{align}\n",
    "\n",
    "The logit function\n",
    "\\begin{align}\\tag{4.3}\n",
    "\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}\n",
    "\\end{align}\n",
    "\n",
    "__SOLUTION__  \n",
    "\\begin{align}\n",
    "p(X) &= \\frac{ e^{\\beta_0 + \\beta_1 X } } { 1 + e^{ \\beta_0 + \\beta_1 X } } \\\\\n",
    "\\\\\n",
    "p(X) + p(X)e^{\\beta_0 + \\beta_1 X } &= e^{\\beta_0 + \\beta_1 X } \\\\\n",
    "\\\\\n",
    "p(X) &= \\left( 1 - p(X) \\right)e^{\\beta_0 + \\beta_1 X } \\\\\n",
    "\\\\\n",
    "\\frac{p(X)}{1 - p(X)} &= e^{\\beta_0 + \\beta_1 X }\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76bf82-7989-40cb-81cb-bee39c01d89e",
   "metadata": {},
   "source": [
    "__2.__ It was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the _k_ th class are drawn from a $N(\\mu_k, \\sigma^2)$ distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.  \n",
    "\n",
    "\\begin{align}\\tag{4.17}\n",
    "p_k(x) = \\frac{ \\pi_k \\frac{ 1 }{ \\sqrt{2 \\pi \\sigma } } \\text{exp} \\left( -\\frac{1}{2 \\sigma^2} (x - \\mu_k)^2 \\right) } \n",
    "              { \\sum^K_{l=1}{\\pi_l} \\frac{1}{\\sqrt{2 \\pi \\sigma} } \\text{exp} \\left(-\\frac{1}{2 \\sigma^2} (x - \\mu_l)^2 \\right) }\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\\tag{4.18}\n",
    "\\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log{(\\pi_k)}\n",
    "\\end{align}\n",
    "\n",
    "__SOLUTION__  \n",
    "\\begin{align}\n",
    "p_k(x) &= \\frac{ \\pi_k \\frac{ 1 }{ \\sqrt{2 \\pi \\sigma } } \\text{exp} \\left( -\\frac{1}{2 \\sigma^2} (x - \\mu_k)^2 \\right) } \n",
    "              { \\sum^K_{l=1}{\\pi_l} \\frac{1}{\\sqrt{2 \\pi \\sigma} } \\text{exp} \\left(-\\frac{1}{2 \\sigma^2} (x - \\mu_l)^2 \\right) }\n",
    "       = \\frac{ \\pi_k e^{ -(1/2\\sigma^2)(x - \\mu_k)^2 } }{ \\sum^K_{l=1}{\\pi_l e^{-(1/2\\sigma^2)(x - \\mu_k)^2 } } } \\\\\n",
    "\\\\\n",
    "\\log{p_k(x)} &= \\log{\\pi_k} - (1/2\\sigma^2)(x - \\mu_k)^2 - \\log{\\sum^K_{l=1}{\\pi_l e^{-(1/2\\sigma^2)(x - \\mu_k)^2 }}} \\\\\n",
    "\\end{align}\n",
    "The last term is independent of $k$\n",
    "\\begin{align}\n",
    "\\log{\\pi_k} - (1/2\\sigma^2)(x - \\mu_k)^2 = \\log{\\pi_k} - \\frac{1}{2\\sigma^2}x^2 + \\frac{\\mu_k}{\\sigma^2}x - \\frac{\\mu_k^2}{2\\sigma^2}\n",
    "\\end{align}\n",
    "This reduces to finding $k$ for which:\n",
    "\\begin{align}\\tag{4.18}\n",
    "\\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log{(\\pi_k)}\n",
    "\\end{align}\n",
    "is largest.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d5591a-006a-4f20-9ea1-e3091bbdd9a2",
   "metadata": {},
   "source": [
    "__3.__ This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class specific mean vector and a class specific covariance matrix. We consider the simple case where $p=1$; i.e. there is only one feature.  \n",
    "\n",
    "Suppose that we have $K$ classes, and that if an observation belongs to the _k_ th class then $X$ comes from a one-dimensional normal distribution, $X \\sim N(\\mu_k, \\sigma_k^2)$. Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is _not_ linear. Argue that it is in fact quadratic.  \n",
    "\n",
    "_Hint: For this problem, you should follow the arguments laid out in Section 4.4.1, but without making the assumption that $\\sigma_1^2 = \\ldots = \\sigma_k^2$_.  \n",
    "\n",
    "__SOLUTION__  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
