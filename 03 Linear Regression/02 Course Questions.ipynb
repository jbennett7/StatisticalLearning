{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc0ce4b-523a-426a-b17b-21165b4cdd94",
   "metadata": {},
   "source": [
    "# Chapter 3 Linear Regression\n",
    "\n",
    "## 3.1 Simple Linear Regression\n",
    "\n",
    "### R.1\n",
    "Why is linear regression important to understand? Select all that apply:\n",
    "\n",
    "1. The linear model is often correct.\n",
    "2. Linear regression is very extensible and can be used to capture nonlinear effects.\n",
    "3. Simple methods can outperform more complex ones if the data are noisy.\n",
    "4. Understanding simpler methods sheds light on more complex ones.\n",
    "\n",
    "Answer: 2, 3, 4\n",
    "The linear model (and every other model) is hardly ever true, but it is an importnat piece in many more complex methods.\n",
    "\n",
    "### R.2\n",
    "You may want to reread the paragraph on confidence intervals on page 66 of the textbook before trying this question (the distinctions are subtle).\n",
    "\n",
    "Which of the following are true statements? Select all that apply:\n",
    "\n",
    "1. A $95%$ confidence interval is a random interval that contains the true parameter $95%$ of the time.\n",
    "2. The true parameter is a random value that has $95%$ chance of falling in the $95%$  confidence interval.\n",
    "3. I perform a linear regression and get a $95%$ confidence interval from $0.4$ to $0.5$. There is a $95%$ probability that the true parameter is between $0.4$ and $0.5$.\n",
    "4. The true parameter (unknown to me) is $0.5$. If I sample data and construct a $95%$ confidence interval, the interval will contain $0.5$ $95%$ of the time.\n",
    "\n",
    "Answer: 1, 4\n",
    "Confidence intervals are a \"frequentist\" concept: the interval, and not the true parameter, is considered random.\n",
    "\n",
    "\n",
    "## 3.2 Hypothesis Testing and Confidence Intervals\n",
    "\n",
    "### R.1\n",
    "We run a linear regression and the slope estimate is $0.5$ with estimated standard error of $0.2$. What is the largest value of $b$ for which we would NOT reject the null hypothesis that $\\beta_1 = b$? (assume normal approximation to $t$ distribution, and that we are using the $5%$ significance level for a two-sided test; need two significant digits of accuracy.\n",
    "\n",
    "Answer: 0.892\n",
    "\n",
    "The $95%$ confidence interval $\\hat{\\beta}_1 \\pm 1.96 \\text{S.E.}(\\hat{\\beta}_1$ contains all parameter values that would not be rejected at a $5%$ significance level.\n",
    "\n",
    "### R.2\n",
    "Which of the following indicates a fairly strong relationship between $X$ and $Y$?\n",
    "\n",
    "1. $R^2 = 0.9$\n",
    "2. The p-value for the null hypothesis $\\beta_1 = 0$ is $0.0001$.\n",
    "3. The t-statistic for the null hypothesis $\\beta_1 = 0$ is $30$.\n",
    "\n",
    "Answer: 1\n",
    "The $R^2$ is the correlation between the two variables and measures how closely they are associated. The p value and the t statistic merely measure how strong is the evidence that there is a nonzero association. Even a week effect can be extremely significant given enough data.\n",
    "\n",
    "## 3.3 Multiple Linear Regression\n",
    "### R.1\n",
    "Suppose we are interested in learning about a relationship between $X_1$ and $Y$, which we would ideally like to interpret as causal.\n",
    "\n",
    "True or False? The estimate $\\hat{\\beta}_1$ in a linear regression that controls for many variables (that is, a regression with many predictors in addition to $X_1$) is usually a more reliable measure of a causal relationship that \\beta_1 from a univariate regression on $X_1$.\n",
    "\n",
    "Answer: False\n",
    "Adding lots of extra predictors to the model can just as easily muddy the interpretation of $\\hat{\\beta}_1$ as it can clarify it. One often reads in media reports of academic studies that \"the invetigators controlled for confounding variables,\" but be skeptical!\n",
    "\n",
    "Causal inference is a difficult and slipery topic, which cannot be answered with observational data alone without additional asusmptions.\n",
    "\n",
    "## 3.4 Some important questions\n",
    "### R.1\n",
    "According to the balance vs ethnicity model, what is the predicted balance for an Asian in the data set? (within 0.01 accuracy)\n",
    "\n",
    "Answer: $512.31$  \n",
    "For an Asian, the predicted balance is the intercept plus the Asian ethnicity effect.\n",
    "\n",
    "### R.2\n",
    "What is the predicted balance for an African American? (within $.01$ accuracy)\n",
    "\n",
    "Answer: $531.00$  \n",
    "For an African American, the predicted balance is just the intercept. Not that dispite the differeng predictions, this difference is not statistically significant.\n",
    "\n",
    "## 3.5 Extensions of the linear model\n",
    "### R.1\n",
    "According to the model for sales vs TV interacted with radio, what is the effect of an additional $\\$1$ of radio advertising if $\\text{TV} = \\$50$? (with 4 decimal accuracy)\n",
    "\n",
    "Answer: $0.0839$\n",
    "\n",
    "### R.2\n",
    "What if $\\text{TV} = \\$250$? (with 4 decimal accuracy)\n",
    "\n",
    "Answer: $.3039$\n",
    "The effect of an additional unit of radio is $.0289$ plus $.0011$ times TV.\n",
    "\n",
    "## 3.R Linear Regression in R\n",
    "### R.1\n",
    "What is the difference between `lm(y~x*z)` and `lm(y~I(x*z))`, when `x` and `z` are both numeric variables?\n",
    "1. The first one includes an interaction term between `x` and `z`, whereas the second uses the product of `x` and `z` as a predictor in the model.\n",
    "2. The second one includes an interaction term between `x` and `z`, whereas the first uses the product of `x` and `z` as a predictor in the model.\n",
    "3. The first includes only an interaction term for `x` and `z`, while the second includes both interaction affects and main effects.\n",
    "4. The second includes only an interaction term for `x` and `z`, while the first includes both interaction effects and main effects.\n",
    "\n",
    "Answer: 1  \n",
    "An interaction term between a numeric `x` and `z` is just the product of `x` and `z`. The difference in the first model, `lm` processes the `*` operator between variables and automatically includes main effects, whereas in the latter model, the expression inside the `I()` function is not parsed as a part of the formula, but rather is simply evaluated.\n",
    "\n",
    "## Chapter 3 Quiz\n",
    "### Q.1\n",
    "Which of the following statements are true?\n",
    "1. In the balance vs. income * student model plotted on slide 44, the estimate of beta3 is negative.\n",
    "2. One advantage of using linear models is that the true regression function is often linear.\n",
    "3. If the F statistic is significant, all of the predictors have statistically significant effects.\n",
    "4. In a linear regression with several variables, a variable has a positive regression coefficient if and only if its correlation with the response is positive.\n",
    "\n",
    "Answer: 1  \n",
    "We can see that the estimate of beta3 is negative because the slope of the student line is smaller than the slope of the non-student line. That is, being a student diminishes the effect of income on balance. The linear model is almost always wrong; however, it is often still useful. The Fstatistic tests the null hypothesis that none of the predictors has any effect. Rejecting that null means concluding that *some* predictor has an effect, not that *all* of them do. Positive corelation only means that the univariate regression has a positive correlation. See slide 20 for a counterexample."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
