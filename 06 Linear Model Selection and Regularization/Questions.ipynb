{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe02dea5-02a5-4c36-9f47-061dc91d2ef1",
   "metadata": {},
   "source": [
    "# Chapter 5 Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b873f-cc07-4f19-9b69-5fa9dd7d414a",
   "metadata": {},
   "source": [
    "## 5.1 Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6671d-0320-4471-a459-c5324cda48a0",
   "metadata": {},
   "source": [
    "### R1\n",
    "When we fit a model to data, which is typically lager?  \n",
    "1. Test Error\n",
    "2. Training Error\n",
    "\n",
    "Answer 1, Training error almost always underestimates test error, sometimes dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3380ec-7cbf-4dac-b263-34dea75d7854",
   "metadata": {},
   "source": [
    "### R2\n",
    "What are reasons why test error could be LESS than training error?\n",
    "1. By chance, the test set has easier cases than the training set.\n",
    "2. The model is highly complex, so training error systematically overestimates test error.\n",
    "3. The model is not very complex, so training error systematically overestimates test error.\n",
    "\n",
    "Answer: 1, Training error usually UNDERestimates test error when the model is very complex (compared ot the training set size), and is a pretty good estimate when the model is not very complex. However, it's always possible we just get too few hard-to-predict points in the test set, or too many in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015257e2-7d76-44cf-a7a7-f6248fb67738",
   "metadata": {},
   "source": [
    "## 5.2  K-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd21e6-642d-49a7-b8c9-624472374bfb",
   "metadata": {},
   "source": [
    "### R1\n",
    "Suppose we want to use cross-validation to estimate the error of the following procedure:\n",
    "Step 1: Find the k variables most correlated with y\n",
    "Step 2: Fit a linear regression using those variables as predictors  \n",
    "We will estimate the error for each k from 1 to p, and then chosse the best k.  \n",
    "\n",
    "True or false: a correct cross-validation procedure will possibly choose a different set of k variables for every fold.  \n",
    "\n",
    "Answer: True: we need to replicate our entire procedure for each training/validation split. That means the decision about which k variables are the best must be made on the basis of the training set along. In general, different training sets will disagree on which are the best k variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54636e0-0a40-4a4e-8e8a-d923117468be",
   "metadata": {},
   "source": [
    "## 5.3 Cross-Validation the wrong and right way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70edc4fc-4d61-4190-ad08-024a577fd5f5",
   "metadata": {},
   "source": [
    "### R1\n",
    "Suppose that we perform forward stepwise regression and use cross-validatio to choose the best model size.  \n",
    "\n",
    "Using the full data set to choose the sequence of models is the WRONG way to do cross-validation (we need to redo the model selection step within each training fold). If we do cross-validation the WRONG way, which of the following is true?  \n",
    "\n",
    "1. The selected model will probably be too complex\n",
    "2. The selected model will probably be too simple\n",
    "\n",
    "Answer: 1: Using the full data set to choose the best variables means that we do not pay as much price as we should for overfitting (since we are fitting to the test and training set simultaneously). This will lead us to underestimate test error for every model size, but the bias is worst for the most complex models. Therefore, we are likely to choose a model that is more complex than the optimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b01fa-e367-4766-8612-cf6fd8b598ef",
   "metadata": {},
   "source": [
    "## 5.4 The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d753b3-178c-4e89-b1f6-f6460008a157",
   "metadata": {},
   "source": [
    "### R1\n",
    "One way of carrying out the bootstrap is to average equally over all possible bootstrap samples from the original data set (where two bootstrap data sets are different if they have the same data points but in different order). Unlike the usual implementation of the bootstrap, this method has the advantage of not introducing extra noise due to resampling randomly. (You can use \"^\" to denote power, as in \"n^2\")  \n",
    "\n",
    "To carry out this implementation on a data set with n data poins, how many bootstrap data sets would we need to average over?  \n",
    "\n",
    "Answer: n^n, completely removing the bootstrap resampling noise is usually not worth incurring the extreme computational cost. If B is large, but still less than n^n, random resampling gives a good Monte Carlo estimate of the idealized bootstrap estimate for all n^n data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292799e0-3b1c-435e-a0ba-ed824519342c",
   "metadata": {},
   "source": [
    "## 5.5 More on the Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea1f00-53e8-48ca-9bc2-972891925ecf",
   "metadata": {},
   "source": [
    "### R1\n",
    "If we have n data points, what is the probability that a given data point does not appear in a bootstrap sample?  \n",
    "\n",
    "Answer: (1-1/n)^n, To construct a bootstrap sample, we repeatedly draw a single data point from a sample of size n, n times. Any given data point has a 1-1/n chance of not being selected in each draw. Hence, the chance of not being selected in any of the n draws is (1-1/n)^n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e0085f-8e3a-4723-874e-3649b9058964",
   "metadata": {},
   "source": [
    "## 5.R Resampling in R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aba98d-0cfb-4d62-ad39-411e5a85e852",
   "metadata": {},
   "source": [
    "### R1\n",
    "Download the file 5.R.RData and load it into R using load(\"5.R.RData\"). Consider the linear regression model of y on X1 and X2. What is the standard error for $\\beta_1$?  \n",
    "\n",
    "Answer: 0.02593, Use summary(lm(y~., data=Xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fddaa4-fec9-4008-9936-8333171e52c8",
   "metadata": {},
   "source": [
    "### R2\n",
    "Next, plot the data using matplot(Xy, type = \"I\"). Which of the following do you think is most likely given what you see?  \n",
    "\n",
    "1. Our estimate of $\\mathrm{S.E.}(\\hat{\\beta}_1)$ is too high.\n",
    "2. Our estimate of $\\mathrm{S.E.}(\\hat{\\beta}_1)$ is too low.\n",
    "3. Our estimate of $\\mathrm{S.E.}(\\hat{\\beta}_1)$ is about right.\n",
    "\n",
    "Answer: 2, There is very strong autocorrelation between consecutive rows of the data matrix. Roughly speaking, we have about 10-20 repeats of every data point, so the sample size is in effect much smaller than the number of rows (1000 in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7601c5-2b4e-414a-8598-81bf27100af3",
   "metadata": {},
   "source": [
    "### R3\n",
    "Now, use the (standard) bootstrap to estimate $\\mathrm{S.E.}(\\hat{\\beta}_1)$. To within 10%, what do you get?  \n",
    "\n",
    "Answer: 0.02844047, When we do the i.i.d. bootstrap, we are relying on the original sampling having been i.i.d. That is the same assumption that screwed us up when we used lm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d667fe-0b8e-4557-a55a-087daf4e1765",
   "metadata": {},
   "source": [
    "### R4\n",
    "Finally, use the block bootstrap to estimate the $\\mathrm{S.E.}(\\hat{\\beta}_1). Use blocks of 100 contiguous observations, and ressample ten whole blocks with replacement then paste them together to construct each bootstrap time series. For example, one of your bootstrap resamples could be:  \n",
    "\n",
    "new.rows = c(101:200, 401:500, 101:200, 901:1000, 301:400, 1:100, 1:100, 801:900, 201:300, 701:800)  \n",
    "\n",
    "```new.Xy = Xy[new.rows,]```\n",
    "\n",
    "To within 10%, what do you get?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea5fec-7574-4197-86b5-3addd1132494",
   "metadata": {},
   "source": [
    "## Chapter 5 Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca0784f-f653-4536-bbe8-3f2ac99a5e3e",
   "metadata": {},
   "source": [
    "### Q1\n",
    "If we use ten-fold cross-validation as a means of model selection, the cross-validation estimate of test error is:  \n",
    "1. biased upward\n",
    "2. biased downward\n",
    "3. unbiased\n",
    "4. potentially any of the above\n",
    "\n",
    "Answer: 4, There are competing biases: on one hand, the cross-validated estimate is based on models trained on smaller training sets than the full model, which means we will tend to overestimate test error for the full model.  \n",
    "\n",
    "On the other hand, cross-validation gives a noisy estimate of test error for each candidate model, and we select the model with the best estimate. This means we are more liekly to choose a model whose estimate is smaller than its true test error rate, hence, we may underestimate test error. In any given case, either source of bias may dominate the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4db88c-604f-4b36-9048-0a6d8bf8a04e",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Why can't we use the standard bootstrap for some time series data?\n",
    "\n",
    "1. The data points in most time series aren't i.i.d.\n",
    "2. Some points will be used twice in the same sample\n",
    "3. The standard bootstrap doesn't accurately mimic the real-world data-generating mechanism.\n",
    "\n",
    "Answer: 1, 3; The bootstrap always involves using some points more than once in each resample, but that doesn't inherently make it incorrect (unless we are trying to gauge prediction error). The real problem in this case is that the usual bootstrap algorithm samples i.i.d., so there is no serial autocorrelation (unlike what is observed in most time series). This makes the set of resampled time series very very different from the sorts of time series we actually get in the real world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
