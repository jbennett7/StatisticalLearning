{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f06838-9f92-458f-ad7a-16a35e662ea5",
   "metadata": {},
   "source": [
    "# 10 Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9eb3a4-d816-45a4-a432-f2eba6631b96",
   "metadata": {},
   "source": [
    "## 10.1 Single Layer Neural Networks\n",
    "![Figure 10.1](SLNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88966f5b-238c-4fb4-a8d3-752d0cafd881",
   "metadata": {},
   "source": [
    "A neural network takes an input vector of $p$ variables $X=(X_1, X_2, \\ldots, X_p)$ and builds a nonlinear function $f(X)$ to predict the response $Y$. The figure above shows a simple _feed-forward neural network_ for modeling a quantitative response using $p=4$ predictors. The four features $X_1, \\ldots, X_4$ make up the units in the _input layer_. The arrows indicate that each of the inputs from the input layer feeds into each of the $K$ _hidden units_ (we  get to pick $K$; here we chose $5$). The neural network model has the form\n",
    "\n",
    "\\begin{align*}\n",
    "f(X) &= \\beta_0 + \\sum^K_{k=1}{\\beta_k h_k(X)} \\\\\n",
    "     &= \\beta_0 + \\sum^K_{k=1}{\\beta_k g(w_{k0} + \\sum^p_{j=1}{w_{kj}X_j})}\\text{.} \\tag{10.1}\n",
    "\\end{align*}\n",
    "\n",
    "It is built up here in two steps. First the $K$ _activations_ $A_k, k=1, \\ldots,K$, in the hidden layer are computed as functions of the input features $X_1,\\ldots,X_p$.\n",
    "\n",
    "\\begin{align*}\n",
    "A_k = h_k(X) = g(w_{k0} + \\sum^p_{j=1}{w_{kj}X_j})\\text{,}\\tag{10.2}\n",
    "\\end{align*}\n",
    "\n",
    "where $g(z)$ is a nonlinear _activation function_ that is specified in advance. We can think of each $A_k$ as a different transformation $h_k(X)$ of the original features, much like the basis functions of Chapter 7. These $K$ activations from the hidden layer then feed into the output layer, resulting in\n",
    "\n",
    "\\begin{align*}\n",
    "f(X) = \\beta_0 + \\sum^K_{k=1}{\\beta_kA_k}\\text{,}\\tag{10.3}\n",
    "\\end{align*}\n",
    "\n",
    "a linear regression model in the $K=5$ activations. ALl the parameters $\\beta_0,\\ldots,\\beta_K$ and $w_{10},\\ldots,w_{Kp}$ need to be estimated from data. In the early instances of neural networks, the _sigmoid_ activation function was favored,\n",
    "\n",
    "\\begin{align*}\n",
    "g(z) = \\frac{e^z}{1 + e^z} = \\frac{1}{1 + e^{-z}}\\text{,}\\tag{10.4}\n",
    "\\end{align*}\n",
    "\n",
    "which is the same function used in logistic regression to convert a linear function into probabilities between zero and one. The preferred choice in modern neural networks in the _ReLU_ (_rectified linear unit_) activation function, which takes the form\n",
    "\n",
    "\\begin{align*}\n",
    "g(z) = (z)_+= \\begin{cases}\n",
    "      0 & \\text{if}\\ z<0 \\\\\n",
    "      z & \\text{otherwise}\n",
    "\\end{cases}\\tag{10.5}\n",
    "\\end{align*}\n",
    "\n",
    "A ReLU activation can be computed and stored more efficiently than a sigmoid activation. Although it thresholds at zero, because we apply it to a linear function (10.2) the constant term $w_{k0}$ will shift this inflection point.\n",
    "\n",
    "So in words, the model depicted in the figure above derives five new features by computing five different linear combinations of $X$, and then squashes each through an activation function $g(\\cdot)$ to transform it. The final model is linear in these derived variables.\n",
    "\n",
    "The name _neural network_ orginally derived from thinking of these hidden units as analogous to neurons in the brain &mdash; values of the activations $A_k = h_k(X)$ close to one are _firing_, while those close to zero are _silent_ (using the sigmoid activation function).\n",
    "\n",
    "The nonlinearity in the activation function $g(\\cdot)$ is essential, since without it the model $f(X)$ in (10.1) would collapse into a simple linear model in $X_1,\\ldots,X_p$. Moreover, having a nonlinear activation function allows the model to capture complex nonlinearities and interaction effects. Consider a very simple example with $p=2$ input variables $X=(X_1, X_2)$, and $K=2$ hidden units $h_1(X)$ and $h_2(X)$ with $g(z)=z^2$. We specify the other parameters as\n",
    "\n",
    "\\begin{align*}\n",
    "\\beta_0 = 0\\text{,}\\ \\ \\ \\ & \\beta_1 = \\frac{1}{4}\\text{,} & \\beta_2 = -\\frac{1}{4}\\text{,} \\\\\n",
    "w_{10} = 0\\text{,}\\ \\ \\ \\ & w_{11} = 1\\text{,} & w_{12} = 1\\text{,} \\\\\n",
    "w_{20} = 0\\text{,}\\ \\ \\ \\ & w_{21} = 1\\text{,} & w_{22} = -1\\text{.}\\tag{10.6}\n",
    "\\end{align*}\n",
    "\n",
    "From (10.2), this means that\n",
    "\n",
    "\\begin{align*}\n",
    "h_1(X) &= (0 + X_1 + X_2)^2\\text{,} \\\\\n",
    "h_2(X) &= (0 + X_1 - X_2)^2\\text{.}\\tag{10.7}\n",
    "\\end{align*}\n",
    "\n",
    "Then plugging (10.7) into (10.1), we get\n",
    "\n",
    "\\begin{align*}\n",
    "f(X) &= 0 + \\frac{1}{4} \\cdot (0 + X_1 + X_2)^2 - \\frac{1}{4} \\cdot (0 + X_1 + X_2)^2 \\\\\n",
    "     &= \\frac{1}{4}\\left[(X_1 + X_2)^2 - (X_1 - X_2)^2\\right] \\\\\n",
    "     &= X_1X_2\\text{.}\\tag{10.8}\n",
    "\\end{align*}\n",
    "\n",
    "So the sum of two nonlinear transformations of linear functions can give us an interaction! In practice we would not use a quadratic function for $g(z)$, since we would always get a second-degree polynomial in the original coordinates $X_1,\\ldots,X_p$. The sigmoid or ReLU activations do not have such a limitation.\n",
    "\n",
    "Fitting a neural network requires estimating the unkown parameters in (10.1). FOr a quantitative response, typically squared-error loss is used, so that the parameters are chosen to minimize\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum^n_{i=1}{(y_i - f(x_i))^2}\\text{.}\\tag{10.9}\n",
    "\\end{align*}\n",
    "\n",
    "Details about how to perform this inimization are provided in Section 10.7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e66bc3-e9fd-417d-bb52-a235fcb2ce5e",
   "metadata": {},
   "source": [
    "## 10.2 Multilayer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c9624-b8c1-4bad-bba6-7484fe24f7d9",
   "metadata": {},
   "source": [
    "## 10.3 Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c6716-4815-4d3b-a076-4fcce7ddd859",
   "metadata": {},
   "source": [
    "### 10.3.1 Convolution Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf8a80-e235-4b31-abff-68a9d12c4d9b",
   "metadata": {},
   "source": [
    "### 10.3.2 Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940287dd-4cac-46da-ac0b-ec25893ece12",
   "metadata": {},
   "source": [
    "### 10.3.3 Architecture of a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe85a4-c5a9-41b4-a289-8006d22dc8f0",
   "metadata": {},
   "source": [
    "### 10.3.4 Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f5965-dc29-49f8-b211-b4483c011e94",
   "metadata": {},
   "source": [
    "### 10.3.5 Results Using a Pretrained Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf5122-da0d-4d7f-be80-c54714c00159",
   "metadata": {},
   "source": [
    "## 10.4 Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcaad93-9aa4-4d43-a88f-3efffe41bde6",
   "metadata": {},
   "source": [
    "## 10.5 Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd149fb1-456b-46cc-a13b-9c01cd2d246c",
   "metadata": {},
   "source": [
    "### 10.5.1 Sequential Models for Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cac5c5-8138-4ce1-a8b8-afe1b67490bb",
   "metadata": {},
   "source": [
    "### 10.5.2 Time Series Forcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167d8eee-13bb-4328-b08a-feea6a3d7ffa",
   "metadata": {},
   "source": [
    "### 10.5.3 Summary of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3d4f7-5360-40fa-95a6-aae71a76648d",
   "metadata": {},
   "source": [
    "## 10.6 When to Use Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115d3199-06ce-457f-a46d-fdd598be7d86",
   "metadata": {},
   "source": [
    "## 10.7 Fitting a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e646557f-3dd8-414f-b8ce-a720f38a0485",
   "metadata": {},
   "source": [
    "### 10.7.1 Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27e8a2-cbce-48a3-8f8b-3ea18ef06c62",
   "metadata": {},
   "source": [
    "### 10.7.2 Regularization and Stochasic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230000a7-31c7-4e14-94f3-694cdb25eaed",
   "metadata": {},
   "source": [
    "### 10.7.3 Dropout Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476fa6ff-de6f-4ec6-b701-478bdb6793ef",
   "metadata": {},
   "source": [
    "## 10.8 Interpolation and Double Descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
